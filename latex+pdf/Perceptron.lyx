#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ctex-article
\begin_preamble
% 如果没有这一句命令，XeTeX会出错，原因参见
% http://bbs.ctex.org/viewthread.php?tid=60547
\DeclareRobustCommand\nobreakspace{\leavevmode\nobreak\ }
\end_preamble
\options UTF8
\use_default_options true
\maintain_unincluded_children false
\language chinese-simplified
\language_package none
\inputencoding utf8-plain
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
感知器 Perceptron
\end_layout

\begin_layout Author
bxf_hit@163.com
\end_layout

\begin_layout Section
基本概念
\end_layout

\begin_layout Standard
感知器模型是Rosenblatt(1962)提出的一个线性判别模型，它在模式识别算法历史上占有重要的意义；感知器对应一个2分类的模型，在这个模型中，输入向量X先
是使用一个固定的非线性变化得到另一个向量
\begin_inset Formula $\text{\phi(x)}$
\end_inset

,这个向量然后被用于构造一个一般的线性模型，形式为
\begin_inset Formula 
\begin{equation}
y(x)=f(w^{T}\phi(x))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中，这里的
\begin_inset Formula $\text{\phi(x)}\text{为}$
\end_inset

n+1维，
\begin_inset Formula $w^{T}\phi(x)$
\end_inset

可以写成
\begin_inset Formula $a^{T}x+b\text{型}$
\end_inset

，非线性激活函数f(t)是一个阶梯函数，形式为
\begin_inset Formula 
\begin{equation}
f(t)=\left\{ \begin{array}{c}
+1,t\geq0\\
-1,t<0
\end{array}\right.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
在之前对于二分类问题的讨论中,我们对于目标变量的表示方法为
\begin_inset Formula $f(t)\text{∈}{0,1}$
\end_inset

,这对于概率模型来说是很合适的。然而,对于感知器来说,更方便的做法是使用
\begin_inset Formula $t=+1$
\end_inset

表示C1 ,使用
\begin_inset Formula $t=−1$
\end_inset

表示C2 ,这与激活函数的选择相匹配。 
\end_layout

\begin_layout Standard
感知器的目的就是找到一个解向量w，可以正确划分所有正例和反例。也就是在空间中寻找一个超平面
\begin_inset Formula $a^{T}x+b=0$
\end_inset

,使之可以划分空间中的两类点。
\end_layout

\begin_layout Section
感知器准则函数
\end_layout

\begin_layout Standard
为了求解解向量w,我们需要定义一个误差函数，误差函数的 一个自然的选择是误分类的模式的总数。但是,这样做会使得学习算法不会很简单,因为这样 做会使误差函数变为w
的分段常函数,从而当w的变化使得决策边界移过某个数据点时,这个 函数会不连续变化。这样做还使得使用误差函数改变w的方法无法使用,因为在几乎所有的地方梯度都等于零
。
\end_layout

\begin_layout Standard
因此我们考虑一个另外的误差函数,被称为感知器准则(perceptron criterion)。为了推导这个函数,我们注意到我们正在做的是寻找一个权向量w使得对于
类别C1中的模式
\begin_inset Formula $x_{n}$
\end_inset

都有
\begin_inset Formula $w^{T}φ(x_{n})>0$
\end_inset

,而对于类别C2中的模式
\begin_inset Formula $x_{n}$
\end_inset

都有
\begin_inset Formula $w^{T}φ(x_{n})<0$
\end_inset

。使用
\begin_inset Formula $t∈{−1,+1}$
\end_inset

这种目标变量的表示方法,我们要做的就是使得所有的模式都满足
\begin_inset Formula $w^{T}φ(x_{n})t_{n}>0$
\end_inset

。对于正确分类的模式,感知器准则赋予零误差,而对于误分类的模式
\begin_inset Formula $x_{n}$
\end_inset

,它试着最小化
\begin_inset Formula $−w^{T}\varphi(x_{n})t_{n}$
\end_inset

 。因此,感知器准则为
\begin_inset Formula 
\begin{equation}
EP(w)=-\sum_{n\in M}w^{T}\varphi(x_{n})t_{n}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中，M表示所有误分类的元素x所组成的集合。当且仅当
\begin_inset Formula $EP(w^{*})=minEP(w)=0$
\end_inset

 时，
\begin_inset Formula $w^{*}$
\end_inset

是解向量。这就是Rosenblatt提出的感知器（Perceptron）准则函数
\end_layout

\begin_layout Standard
对于感知器的准则函数，其实还有另一种理解：采用误分类点到超平面的距离（可以自己推算一下，这里采用的是几何间距，就是点到直线的距离）：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
EP(w)=-\frac{1}{\parallel a\parallel}\sum_{n\in M}(a^{T}\phi(x_{n})+b)t_{n}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
之前有提到，
\begin_inset Formula $w^{T}\phi(x)$
\end_inset

可以写成
\begin_inset Formula $a^{T}x+b\text{型}$
\end_inset

，再加上
\begin_inset Formula $\frac{1}{\text{\parallel a\parallel}}$
\end_inset

可以归一化,就可以得到与公式（3）相同的感知器准则函数了。
\end_layout

\begin_layout Standard
我们现在对这个误差函数（3）使用随机梯度下降算法，可以得到：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w(t+1)=w(t)-η\text{∇}EP(w)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中，t为迭代次数，
\begin_inset Formula $η$
\end_inset

为调整的步长。即下一次迭代的权向量是把当前时刻的权向量向目标函数的负梯度方向调整一个修正量,修正量可以表示为：
\begin_inset Formula 
\begin{equation}
\text{∇}EP(w)=\frac{\partial EP(w)}{\partial w}=-\sum_{n\in M}\varphi(x_{n})t_{n}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
由（4）（5）可得：
\begin_inset Formula 
\begin{equation}
w(t+1)=w(t)+η\sum_{n\in M}\varphi(x_{n})t_{n}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
我们反复对于训练模式进行循环处理,对于每个模式
\begin_inset Formula $x_{i}$
\end_inset

我们计算感知器函数,如果模式正确分类,那么权向量保持不变,而如果模式被错误分类,那么对于类别C1 ,我们把向量
\begin_inset Formula $φ(x_{i})$
\end_inset

加到当前对于权向量w的估计值上,而对于类别C2,我们从w中减掉向量
\begin_inset Formula $φ(x_{i})$
\end_inset

(这里做了假设
\begin_inset Formula $\eta=1$
\end_inset

)。 
\end_layout

\begin_layout Standard
感知器算法的伪代码可以简单描述如下：
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/jackbai/Pictures/pic1.png

\end_inset


\end_layout

\begin_layout Standard
通常情况，一次将所有错误样本进行修正不是效率最高的做法，更常用是每次只修正一个样本或一批样本的固定增量法：
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/jackbai/Pictures/pic2.png

\end_inset


\end_layout

\begin_layout Section
感知器算法的对偶形式
\end_layout

\begin_layout Standard
我们假设样本点
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

在更新过程中被使用了
\begin_inset Formula $n_{i}$
\end_inset

次，因此，从原始形式的学习过程中可以得到，最后学习到的w和b(这里采用
\begin_inset Formula $w^{T}x+b$
\end_inset

模式)可以分别表示为：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w=\sum_{i=1}^{N}n_{i}\eta y_{i}x_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
b=\sum_{i=1}^{N}n_{i}\eta y_{i}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
考虑
\begin_inset Formula $n_{i}$
\end_inset

的含义：若
\begin_inset Formula $n_{i}$
\end_inset

的值越大，那么意味着这个样本点经常被误分。什么样的样本点容易被误分呢？很明显就是离超平面很近的点，超平面稍微一改动，这些点就会由正变成负，这些点就有可能是支持向
量。
\end_layout

\begin_layout Standard
将（8）（9）带入原始的感知机模型中，可得：
\begin_inset Formula 
\begin{equation}
f(x)=sign(w\cdot x+b)=sign(\sum_{i=1}^{N}n_{i}\eta y_{i}x_{i}\cdot x+\sum_{i=1}^{N}n_{i}\eta y_{i})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
显然此时的学习目标就不是w,b,而变成了
\begin_inset Formula $n_{i}$
\end_inset

,i=1,2
\begin_inset Formula $\cdots$
\end_inset

N
\end_layout

\begin_layout Itemize
训练过程也就可以表示为：
\end_layout

\begin_deeper
\begin_layout Enumerate
初始化
\begin_inset Formula $n_{i}$
\end_inset

=0
\end_layout

\begin_layout Enumerate
训练集中选取数据
\begin_inset Formula $(x_{i},y_{i})$
\end_inset


\end_layout

\begin_layout Enumerate
如果
\begin_inset Formula $y_{i}(\sum_{i=1}^{N}n_{i}\eta y_{i}x_{i}\cdot x+\sum_{i=1}^{N}n_{i}\eta y_{i})\leq0$
\end_inset

,更新
\begin_inset Formula $n_{i}\leftarrow n_{i}+1$
\end_inset


\end_layout

\begin_layout Enumerate
转至2直至没有误分类数据
\end_layout

\end_deeper
\begin_layout Standard
可以看出，其实对偶形式和原始形式本质上没有较大区别，那么对偶形式的意义在哪里呢？从式（10）可以看出，样本点的特征向量以内积的形式存在于感知器对偶形式的训练算法
中，因此如果事先计算好所有的内积，也就是Gram矩阵，就可以较大的提高计算速度。
\end_layout

\begin_layout Standard
对偶形式在求出
\begin_inset Formula $n_{i}$
\end_inset

后，再利用公式（8）（9）即可求得w,b。
\end_layout

\begin_layout Section
感知器收敛定理
\end_layout

\begin_layout Standard
感知器收敛定理(perceptron convergence theorem)表明,如果存在一个精确的解 (即,如果训练数据线性可分),那么感知器算法可以保证在
有限步骤内找到一个精确解。这个定理的证明可以参考Rosenblatt(1962)、 Block(1962)、Nilsson(1965)、Minsky
 and Papert( 1969)、Hertz et al.( 1991)以及Bishop(1995a)。
\end_layout

\begin_layout Section
关于感知器
\end_layout

\begin_layout Itemize
需要注意的是,感知器达到收敛状态所需的步骤数量可能非常大,并且在实际应用中,在达到收敛状态之前,我们不能够区分不可分问题与缓慢收敛问题。 
\end_layout

\begin_layout Itemize
即使数据集是线性可分的,也可能有多个解,并且最终哪个解会被找到依赖于参数的初始化以及数据点出现的顺序。此外,对于线性不可分的数据集,感知器算法永远不会收敛。
 
\end_layout

\begin_layout Itemize
除了学习算法的这些困难之处以外,感知器算法无法提供概率形式的输出,也无法直接推广 到K > 2个类别的情形。然而,最重要的局限性是它基于固定基函数的线性组合
\end_layout

\begin_layout Standard
关于感知器算法更多的局限性,可以参考Minsky and Papert( 1969)和Bishop(1995a)
\end_layout

\begin_layout Section
参考文章
\end_layout

\begin_layout Enumerate
Pattern Recognition And Machine Learning,chapter 4
\end_layout

\begin_layout Enumerate
http://blog.csdn.net/xiaowei_cqu/article/details/9004101
\end_layout

\begin_layout Enumerate
https://www.zhihu.com/question/26526858
\end_layout

\begin_layout Enumerate
统计学习方法-李航
\end_layout

\end_body
\end_document
