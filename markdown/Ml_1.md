title: 机器学习笔记（一）
tags: 机器学习
category: 那些年忘填的坑
---
## 基本概念
### 监督学习（Supervised Learning）
没有找到太正规的定义，基本思想是数据集中的每个样本都有自己的正确答案，我们的任务就是根据已有的训练样本设计一个模型，使这个模型能对于任意合法的输入能产生一个尽量好的输出
<!-- more -->
### 非监督学习（Unsupervised Learning）
无监督学习中的对象没有任何标签或者说是有相同的标签，就是一个数据集，我们要做的就是从中找到某种结构将其中的数据分开
### 统计学习三要素
方法 = 模型 + 策略 + 算法（统计学习方法--李航）
#### 模型
m 代表训练集中实例的数量
n 输入数据特征的个数（#features）
x 代表特征/输入变量
y 代表目标变量/输出变量
(x,y) 代表训练集中的实例
(x(i),y(i) ) 代表第 i 个观察实例
可能的模型的集合称为假设空间
模型中用到的参数所在的n维欧氏空间称为参数空间
#### 策略
有了模型的假设空间，我们需要考虑的是通过什么准则去选择模型，这就是策略的由来
常用损失函数：
* 0-1损失函数
* 平方损失函数
* 绝对损失函数
* 对数损失函数

#### 算法
有了模型的假设空间，有了优化的方向，现在需要做的就是去求解左右的模型了，这就是算法要做的事情：求解参数来使模型最优
### 监督学习过程图解
![监督学习](http://7xigyc.com1.z0.glb.clouddn.com/Image%201.png)
## 第一个例子--单变量线性回归
以房屋交易问题为例：
![房屋交易](http://7xigyc.com1.z0.glb.clouddn.com/Image%202.png)
易得出一个可能的表达式：h = θ0 + θ1*x
### 策略：选择出可以使得建模误差的平方和能够最小的模型参数
### 代价函数：
![代价函数](http://7xigyc.com1.z0.glb.clouddn.com/Image%204.png)
代价函数的一些理解：
我们绘制一个等高线图，三个坐标分别为 θ0 和 θ1 和 J(θ0,θ1)：
我们的目标就是找到那个使 J(θ0,θ1)最小的点
![代价函数](http://7xigyc.com1.z0.glb.clouddn.com/Image%203.png)
### 常见几种求解θ的方法
1.梯度下降法
2.牛顿法
3.解析法
先说这么多，下节详细说下梯度下降法