数学基础☞线性代数
============================

最近打算总结下机器学习过程中用到的线性代数知识,以作备忘.知识主要来源于DeepLearning,PRML

----------

1. 基本数学符号:
----------

> **Scalars(标量)**:  一个标量就是一个单独的数,它不同于线性代数中研究的其他大部分对象
> **向量(vector)**:  一个向量是一列数。这些数是有序排列的。通过次序中的索引,我们可以确定每个单独的数
> **矩阵(matrix)**: 矩阵就是一个2维数组, 如果一个矩阵是阵高度为m,宽度为 n,那么我们说 A ∈ $\mathbb{R}^{m \times n}$。$f (A)_{i,j}$ 表示函数$f$ 作用在 $A$ 上输出的矩阵的第 $i$ 行第$j$ 列元素
> **张量(tensor)**: 在某些情况下,我们会讨论坐标超过两维的数组。一般地,一个数组中的元素分布在若干维坐标的规则网格中,我们称之为张量。
 

2. 矩阵和向量相乘:
-----------

> **矩阵的转置(Transpose)**: $\Rightarrow$穿脱原理 $(AB)^T=B^TA^T$
> **矩阵乘法定义(Matrix(Dot)Product)**: $C= AB \Rightarrow C_{i,j}=\sum\limits_{k} A_{i,k}B_{k,j}$
> **矩阵的Hadamard乘积**: $A\odot B$, 元素对应乘积
> 

3. 单位矩阵和逆矩阵
-----------

	> **单位阵(Identity Matrix)**: 主对角线元素全是1, 满足$\forall x \in \mathbb{R}^{n},I_nx = x$
> **线性方程组**: $Ax = b$,$A$,$b$已知,求解$x$,一个方程的解可以是:
> + No solution
> + Many solutions
> + Exactly one solution: this means multiplication by the matrix is an invertible function.
> 
> **逆矩阵(Matrix Inversion)**: def as $A^{-1}A=I_n$ 
> **矩阵的可逆性(Invertibility)**: Matrix can’t be inverted if…
> + More rows than columns 
> + More columns than rows
> + Redundant rows/columns (“linearly dependent”, “low rank”)

4. 线性相关和生成子空间
-------------

**生成子空间(span)**: 是原始向量线性组合(linear combination)后所能抵达的点的集合, 确定$Ax = b$是否有解相当于确定$b$是否在$A$的列向量张成的生成子空间中.
**线性无关(linear independence)**: 如果一组向量中的任意一个向量都不能表示成其他向量的线性组合,那么这组向量称为 线性无关(linearly independent),反之则是线性相关.
> 再来考虑$Ax=b$的问题,假如b是m维向量,那么要想令方程**对任意$b$有解**, 那么矩阵A的列空间要涵盖整个 $R_{m}$ , 该矩阵必须包含至少一组 m 个线性无关的向量,即**列满秩**.

	**奇异性(singular)**: 一个列向量线性相关的**方阵**被称为 奇异的(singular)

5. 范数(Norms)
------------

有时我们需要衡量一个*向量的大小*。在机器学习中,我们经常使用被称为 **范数(norm)**的函数衡量向量大小, 形式上,$L_p$范数定义如下:
$\parallel x \parallel_p = (\sum\limits_i|x_i|^p)^{\frac{1}{p}}$,其中$p\in \mathbb{R},p \ge 1$
更严格地说,范数是满足下列性质的任意函数:

 - $f (x) = 0 ⇒ x = 0$
 - $f (x + y) ≤ f (x) + f (y)$ ( 三角不等式(triangle inequality))
 - $∀α ∈ R, f (αx) = |α|f (x)$

> $p=2$时,$L_2$ 范数被称为欧几里得范数(Euclidean norm)。它表示从原点出发到向量 $x$ 确定的点的欧几里得距离,常简化为$\parallel x \parallel$,可以通过点积$x^Tx$来计算.
> 另外一个经常在机器学习中出现的范数是 $L∞$ 范数,也被称为 最大范数(max norm)。这个范数表示向量中具有**最大幅值的元素**的绝对值: 
> $\parallel x\parallel_\infty=\max\limits_{i}|x_i|$
> 有时候我们可能也希望衡量矩阵的大小。在深度学习中,最常见的做法是使用 Frobenius 范数(Frobenius norm),
> $\parallel A\parallel_F=\sqrt {\sum\limits_{i,j}A^2_{i,j}}$

6. 特殊的矩阵和向量
-----------
> **对角阵(diagonal matrix)**: 对于所有的$ i \neq j,D_{i,j} = 0$
$diag(v)^{−1} = diag([\frac{1}{v_1} , . . . , \frac{1}/{v_n} ]^⊤)$
> **对称阵(symmetric matrix)**: $A^T = A$, 当某些**不依赖参数顺序的双参数函数**生成元素时,对称矩阵经常会出现。例如,如果 A 是一个距离度量矩阵,$A_{i,j}$ 表示点$i$  到点 $j$的距离,那么 $A_{i,j} = A_{j,i}$ ,因为距离函数是对称的。
> **正交阵(orthogonal matrix)**: 是指行向量和列向量是分别**标准正交**(向量不仅互相正交,并且范数都为 1)的方阵.
> $A^⊤ A = AA^⊤ = I \Rightarrow A^{-1} = A^T$

7. 特征分解(eigendecomposition)
-------
**特征分解(eigendecomposition)**是使用最广的矩阵分解之一,即我们将矩阵分解成一组特征向量和特征值。
方阵 $A$ 的 特征向量(eigenvector)是指与 $A$相乘后相当于对该向量进行缩放的非零向量$v$ :
即 $Av = \lambda v$
标量 λ 被称为这个特征向量对应的 **特征值(eigenvalue)**。(类似地,我们也可以定义 **左特征向量(left eigenvector) **$v^⊤ A = λv^⊤$ ,但是通常我们更关注 **右特征向量(right eigenvector)**)。
如果 v 是 A 的特征向量, 那么任何缩放后的向量 s$v ($s $ \in R,$s$  \ne 0)$ 也是 A 的特征向量。此外,s$v$ 和 $v$ 有相同的特征值。基于这个原因,通常我们只考虑单位特征向量。
假 设 矩 阵 A 有 n 个 线 性 无 关 的 特 征 向 量 $V = \{v^{(1)} , . . . , v^{(n)}\}$, 对 应 着 特 征 值$\lambda = \{λ_1 , . . . , λ_n\}$。$A$ 的 特征分解(eigendecomposition)可以记作 $A = Vdiag(λ)V^{−1} $
注意, 不是每一个矩阵都可以分解成特征值和特征向量.
**实对称矩阵的特征分解**: 每个实对称矩阵都可以分解成实特征向量和实特征值: $A= QΛQ^T$ , 其中 Q 是 A 的特征向量组成的正交矩阵,Λ 是对角矩阵。特征值 $Λ_i$,i 对应的特征
向量是矩阵 Q 的第 i 列,记作 $Q_{:,i} $。
>虽然任意一个实对称矩阵 A 都有特征分解,但是特征分解可能并不唯一。如果两个或多个特征向量拥有相同的特征值,那么在由这些特征向量产生的生成子空间中,任意一组正交向量都是该特征值对应的特征向量。因此,我们可以等价地从这些特征向量中构成 Q 作为替代。按照惯例,我们通常按降序排列 Λ 的元素。在该约定下,特征分解唯一当且仅当所有的特征值都是唯一的。

> 矩阵的特征分解给了我们很多关于矩阵的有用信息。矩阵是奇异的当且仅当含有零特征值。实对称矩阵的特征分解也可以用于优化二次方程 $f (x) = x^⊤ Ax$,其中限制 $∥x∥_2 = 1$。当 $x$ 等于$ A$ 的某个特征向量时,$f$ 将返回对应的特征值。在限制条件下,函数 $f$ 的最大值是最大特征值,最小值是最小特征值. **very useful**

> 所有特征值都是正数的矩阵被称为 **正定(positive definite)**;所有特征值都是非负数的矩阵被称为 **半正定(positive semidefinite)**。同样地,所有特征值都是负数的矩阵被称为 **负定(negative definite)**;所有特征值都是非正数的矩阵被称为 **半负定(negative semidefinite)**
 。半正定矩阵受到关注是因为它们保证 $∀x, x^⊤Ax ≥ 0$。此外,
正定矩阵还保证 $x^⊤Ax = 0 ⇒ x = 0$。**useful**

8. 奇异值分解
--------
还有另一种分解矩阵的方法,被称为 **奇异值分解(singular valuedecomposition, SVD)**,将矩阵分解为 **奇异向量(singular vector)**和 **奇异值(singular value)**。通过奇异值分解,我们会得到一些与特征分解相同类型的信息。然而,奇异值分解有更广泛的应用。
> 每个实数矩阵都有一个奇异值分解,但不一定都有特征分解。例如,非方阵的矩阵没有特征分解,这时我们只能使用奇异值分解。

奇异值分解与特征分解类似,只不过这回我们将矩阵 $A\in \mathbb{R}^{m\times n}$ 分解成三个矩阵的乘积:
$A = UDV^T$, 
其中U 是一个 m × m 的矩阵,D 是一个 m × n的矩阵,V 是一个 n × n 矩阵。**矩阵 U 和 V 都为正交矩阵,而矩阵 D 为对角矩阵。注意,矩阵 D 不一定是方阵。**

对角矩阵 D 对角线上的元素被称为矩阵 A 的 **奇异值(singular value)**。矩阵U 的列向量被称为 **左奇异向量(left singular vector)**
 ,矩阵 V 的列向量被称 **右奇异向量(right singular vector)**。

>事实上,我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的 左奇异向量(left singular vector)是$AA^⊤$ 的特征向量。 A 的右奇异向量(right singularvector)是 $A^⊤A$ 的特征向量。A 的非零奇异值是 $A^⊤A $特征值的平方根,同时也是$AA^⊤$ 特征值的平方根。SVD 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上.

9. Moore-Penrose 伪逆
-------------------
对于非方矩阵而言,其逆矩阵没有定义。假设在问题$Ax = y
$中,我们希望通过矩阵 A 的左逆 B 来求解线性方程,则
$ x = By$ , 而取决于问题的形式,我们可能无法设计一个唯一的映射将 A 映射到 B
> 如果矩阵 A 的行数大于列数,那么上述方程可能没有解。如果矩阵 A 的行数小于列数,那么上述矩阵可能有多个解。
**Moore-Penrose 伪逆(Moore-Penrose pseudoinverse)**使我们在这类问题上取得了一定的进展。矩阵 A 的伪逆定义为:
$A^+ = \lim\limits_{a\rightarrow0} (A^⊤ A + αI)^−1 A^⊤.$
而计算伪逆的实际算法没有基于这个定义,而是使用下面的公式:
$A^+ = VD^+U^⊤$
其中,矩阵 U,D 和 V 是矩阵 A奇异值分解后得到的矩阵。对角矩阵 D 的伪逆$D^+$是其非零元素取倒数之后再转置得到的。
> 当矩阵 A 的列数多于行数时,使用伪逆求解线性方程是众多可能解法中的一种。特别地,$x = A^+ y$ 是方程所有可行解中欧几里得范数 $∥x∥_2$ 最小的一个。
当矩阵 A 的行数多于列数时,可能没有解。在这种情况下,通过伪逆得到的 $x$使得 $Ax$ 和 $y$ 的欧几里得距离 $∥Ax − y∥_2$ 最小。

10. 迹运算
-------
**迹(trace)**运算返回的是矩阵对角元素的和:
$Tr(A) = \sum\limits_{i} A_{i,i} .$
。
> 迹运算提供了另一种描述矩阵Frobenius
范数的方式:
$ \sqrt
{∥A∥_F} =Tr(AA^⊤ ).$

**迹的轮转不变性:** 
$Tr(ABC) = Tr(CAB) = Tr(BCA).$
更一般的,
$Tr(\sum\limits_{i=1}^n F^{(i)}) = Tr(F(n)\sum\limits_{i=1}^{n-1}F^{(i)} )$.
即使循环置换后矩阵乘积得到的矩阵形状变了,迹运算的结果依然不变

11. 行列式
-------

行列式,记作 **det(A)**, 是一个将方阵 A 映射到实数的函数。**行列式等于矩阵特征值的乘积**。行列式的绝对值可以用来**衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少**